{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "---\n",
    "In this notebook, we train a **CNN** to classify images from the CIFAR-10 database.\n",
    "\n",
    "The images in this database are small color images that fall into one of ten classes; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for CUDA\n",
    "\n",
    "Since these are larger (32x32x3) images, it may prove useful to speed up your training time by using a GPU. CUDA is a parallel computing platform and CUDA Tensors are the same as typical Tensors, only they utilize GPU's for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load the Data\n",
    "\n",
    "If you're not familiar with the Cifar-10, you may find it useful to look at: http://www.cs.toronto.edu/~kriz/cifar.html . Or you could search it by yourself. \n",
    "\n",
    "If you can't download it online or it takes long time due to the internet, you may use the attachment file provided on the class website\n",
    "\n",
    "#### TODO: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: load the data   #\n",
    "#############################################################################\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "batch_size=2048\n",
    "\n",
    "transform=transforms.Compose(\n",
    "    [transforms.Resize(224),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "full_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform)\n",
    "train_length = int(full_dataset.__len__()*0.8)\n",
    "val_length = full_dataset.__len__() - train_length\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_length, val_length])\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform)\n",
    "\n",
    "transform=transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Resize(224),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0)\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE               #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the Network Architecture\n",
    "\n",
    "This time, you'll define a CNN architecture. You may use \n",
    "* [Convolutional layers](https://pytorch.org/docs/stable/nn.html#conv2d), which can be thought of as stack of filtered images.\n",
    "* [Maxpooling layers](https://pytorch.org/docs/stable/nn.html#maxpool2d), which reduce the x-y size of an input, keeping only the most _active_ pixels from the previous layer.\n",
    "* The usual Linear + Dropout layers to avoid overfitting and produce a 10-dim output.\n",
    "\n",
    "\n",
    "#### Output volume for a convolutional layer\n",
    "\n",
    "To compute the output size of a given convolutional layer we can perform the following calculation:\n",
    "> We can compute the spatial size of the output volume as a function of the input volume size (W), the kernel/filter size (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. The correct formula for calculating how many neurons define the output_W is given by `(W−F+2P)/S+1`. \n",
    "\n",
    "For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output.\n",
    "\n",
    "#### TODO: Define a model with multiple convolutional layers and an output layer for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CnnNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1_1): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (layer1_2): BasicBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (layer2_1): BasicBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2_2): BasicBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (layer3_1): BasicBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3_2): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (layer4_1): BasicBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4_2): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Define a CNN model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#使用ResNet模型，首先定义基本块\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # 经过处理后的x要与x的维度相同(尺寸和深度)\n",
    "        # 如果不相同，需要添加卷积+BN来变换为同一维度\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# define the CNN architecture\n",
    "class CnnNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CnnNet, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: define your own CNN network              #\n",
    "        #############################################################################\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1_1 = BasicBlock(in_planes=64, planes=64, stride=1)\n",
    "        self.layer1_2 = BasicBlock(in_planes=64, planes=64, stride=1)\n",
    "        self.layer2_1 = BasicBlock(in_planes=64, planes=128, stride=2)\n",
    "        self.layer2_2 = BasicBlock(in_planes=128, planes=128, stride=1)\n",
    "        self.layer3_1 = BasicBlock(in_planes=128, planes=256, stride=2)\n",
    "        self.layer3_2 = BasicBlock(in_planes=256, planes=256, stride=1)\n",
    "        self.layer4_1 = BasicBlock(in_planes=256, planes=512, stride=2)\n",
    "        self.layer4_2 = BasicBlock(in_planes=512, planes=512, stride=1)\n",
    "        self.linear = nn.Linear(512, 10)\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.maxpool(out)\n",
    "        out = self.layer1_1(out)\n",
    "        out = self.layer1_2(out)\n",
    "        out = self.layer2_1(out)\n",
    "        out = self.layer2_2(out)\n",
    "        out = self.layer3_1(out)\n",
    "        out = self.layer3_2(out)\n",
    "        out = self.layer4_1(out)\n",
    "        out = self.layer4_2(out)\n",
    "        out = F.adaptive_avg_pool2d(out,output_size=1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        #out = self.dropout(out)\n",
    "        return out\n",
    "        #############################################################################\n",
    "        #                          END OF YOUR CODE               #\n",
    "        #############################################################################\n",
    "\n",
    "model = CnnNet()\n",
    "print(model)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify [Loss Function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [Optimizer](http://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "Decide on a loss and optimization function that is best suited for this classification task. Pay attention to the value for **learning rate** as this value determines how your model converges to a small error.\n",
    "\n",
    "#### TODO: Define the loss and optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: define the loss and optimizer              #\n",
    "#############################################################################\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.001)\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE               #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train the Network\n",
    "\n",
    "Remember to look at how the training and validation loss decreases over time and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1|Train_loss:2.023 Eval_loss:2.043 Train_acc:0.274 Eval_acc:0.255\n",
      "Epoch 2|Train_loss:1.550 Eval_loss:1.775 Train_acc:0.415 Eval_acc:0.357\n",
      "Epoch 3|Train_loss:1.309 Eval_loss:1.716 Train_acc:0.509 Eval_acc:0.415\n",
      "Epoch 4|Train_loss:1.152 Eval_loss:1.340 Train_acc:0.566 Eval_acc:0.509\n",
      "Epoch 5|Train_loss:1.011 Eval_loss:1.334 Train_acc:0.623 Eval_acc:0.517\n",
      "Epoch 6|Train_loss:0.893 Eval_loss:1.102 Train_acc:0.664 Eval_acc:0.588\n",
      "Epoch 7|Train_loss:0.798 Eval_loss:0.967 Train_acc:0.700 Eval_acc:0.647\n",
      "Epoch 8|Train_loss:0.719 Eval_loss:0.852 Train_acc:0.728 Eval_acc:0.677\n",
      "Epoch 9|Train_loss:0.636 Eval_loss:1.230 Train_acc:0.755 Eval_acc:0.618\n",
      "Epoch 10|Train_loss:0.572 Eval_loss:0.772 Train_acc:0.780 Eval_acc:0.720\n",
      "Epoch 11|Train_loss:0.496 Eval_loss:1.068 Train_acc:0.807 Eval_acc:0.654\n",
      "Epoch 12|Train_loss:0.430 Eval_loss:0.992 Train_acc:0.832 Eval_acc:0.676\n",
      "Epoch 13|Train_loss:0.375 Eval_loss:0.902 Train_acc:0.848 Eval_acc:0.695\n",
      "Epoch 14|Train_loss:0.338 Eval_loss:1.091 Train_acc:0.862 Eval_acc:0.678\n",
      "Epoch 15|Train_loss:0.280 Eval_loss:0.912 Train_acc:0.881 Eval_acc:0.715\n",
      "Epoch 16|Train_loss:0.223 Eval_loss:1.037 Train_acc:0.901 Eval_acc:0.688\n",
      "Epoch 17|Train_loss:0.169 Eval_loss:0.927 Train_acc:0.923 Eval_acc:0.732\n",
      "Epoch 18|Train_loss:0.145 Eval_loss:0.983 Train_acc:0.929 Eval_acc:0.719\n",
      "Epoch 19|Train_loss:0.123 Eval_loss:1.003 Train_acc:0.937 Eval_acc:0.727\n",
      "Epoch 20|Train_loss:0.092 Eval_loss:0.775 Train_acc:0.948 Eval_acc:0.779\n",
      "Epoch 21|Train_loss:0.059 Eval_loss:0.771 Train_acc:0.960 Eval_acc:0.770\n",
      "Epoch 22|Train_loss:0.041 Eval_loss:1.366 Train_acc:0.966 Eval_acc:0.689\n",
      "Epoch 23|Train_loss:0.030 Eval_loss:1.208 Train_acc:0.970 Eval_acc:0.729\n",
      "Epoch 24|Train_loss:0.025 Eval_loss:0.835 Train_acc:0.971 Eval_acc:0.782\n",
      "Epoch 25|Train_loss:0.017 Eval_loss:0.655 Train_acc:0.973 Eval_acc:0.813\n",
      "Epoch 26|Train_loss:0.011 Eval_loss:0.770 Train_acc:0.975 Eval_acc:0.802\n",
      "Epoch 27|Train_loss:0.009 Eval_loss:0.695 Train_acc:0.975 Eval_acc:0.816\n",
      "Epoch 28|Train_loss:0.006 Eval_loss:0.683 Train_acc:0.976 Eval_acc:0.817\n",
      "Epoch 29|Train_loss:0.003 Eval_loss:0.637 Train_acc:0.976 Eval_acc:0.829\n",
      "Epoch 30|Train_loss:0.002 Eval_loss:0.610 Train_acc:0.976 Eval_acc:0.833\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: train and validation              #\n",
    "#############################################################################\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "for epoch in range(30):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    train_correct_count = 0\n",
    "    model.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        train_correct_count += (torch.argmax(outputs,dim=1)==labels).sum().cpu().item()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.cpu().item()\n",
    "    train_loss=train_loss/(i+1)\n",
    "    train_acc=train_correct_count/(i+1)/batch_size\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_correct_count = 0\n",
    "    model.eval()\n",
    "    for i, data in enumerate(valloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        val_correct_count += (torch.argmax(outputs,dim=1)==labels).sum().cpu().item()\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.cpu().item()\n",
    "    val_loss=val_loss/(i+1)\n",
    "    val_acc=val_correct_count/(i+1)/batch_size\n",
    "\n",
    "    print('Epoch %d|Train_loss:%.3f Eval_loss:%.3f Train_acc:%.3f Eval_acc:%.3f'%(epoch+1,train_loss,val_loss,train_acc,val_acc))\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE               #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test the Trained Network\n",
    "\n",
    "Test your trained model on previously unseen data and print the test accuracy of each class and the whole! Try your best to get a better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test|Class1(plane)-acc=0.867\n",
      "Test|Class2(car)-acc=0.951\n",
      "Test|Class3(bird)-acc=0.816\n",
      "Test|Class4(cat)-acc=0.72\n",
      "Test|Class5(deer)-acc=0.831\n",
      "Test|Class6(dog)-acc=0.751\n",
      "Test|Class7(frog)-acc=0.896\n",
      "Test|Class8(horse)-acc=0.859\n",
      "Test|Class9(ship)-acc=0.926\n",
      "Test|Class10(truck)-acc=0.889\n",
      "\n",
      "Test|Overall-acc=0.8506\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: test the trained network             #\n",
    "#############################################################################\n",
    "class_count=np.zeros(10,dtype=int)\n",
    "correct_count=np.zeros(10,dtype=int)\n",
    "model.eval()\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    inputs, labels = data\n",
    "    inputs=inputs.to(device)\n",
    "    outputs = torch.argmax(model(inputs),dim=1).cpu().item()\n",
    "    class_count[labels]+=1\n",
    "    if outputs==labels:\n",
    "        correct_count[labels]+=1\n",
    "for i in range(10):\n",
    "    print('Test|Class{}('.format(i+1)+classes[i]+')-acc={}'.format(correct_count[i]/class_count[i]))\n",
    "#此处展示的是各类别的查准率=正确识别数量/该类别样本总数\n",
    "print('\\nTest|Overall-acc={}'.format(np.sum(correct_count/10000)))\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE               #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What are your model's weaknesses during your experiment and how might they be improved?\n",
    "+ 观察最终各类别上的分类准确率，类别之间存在较大的差异。因此可以得知，不同类别的物体特征获取、识别的难度不尽相同。通过随机裁剪、翻转、缩放等方式进行数据增强可以获得模型在不同样本上更好的鲁棒性，有可能提升模型表现。\n",
    "+ 较小规模的网络的表出能力有限而难以取得较好的结果，较大规模的网络训练代价又较大。因此选择合适规模的网络是必要的，事实上，神经网络的表出能力也是机器学习理论研究的前沿方向之一。同时，例如深度可分离卷积等方法也提供了神经轻量化的方法。\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}